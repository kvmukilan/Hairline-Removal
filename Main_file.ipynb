{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kvmukilan/Hairline-Removal/blob/main/Main_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733f3500",
      "metadata": {
        "id": "733f3500",
        "outputId": "df5cc8cd-7508-4c15-bd51-d179e6fdcd37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(450, 600, 3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "src = cv2.imread(\"C:/Users/Karmukilan/OneDrive/Pictures/akiec_0.26413982314989.jpg\")\n",
        "\n",
        "print( src.shape )\n",
        "cv2.imshow('orig', src )\n",
        "cv2.waitKey(0)\n",
        "\n",
        "# Convert the original image to grayscale\n",
        "grayScale = cv2.cvtColor( src, cv2.COLOR_BGR2GRAY )\n",
        "cv2.imshow('grey',grayScale)\n",
        "cv2.waitKey(0)\n",
        "cv2.imwrite('C/Users/Karmukilan/Pictures/grayScale_sample1.jpg', grayScale, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
        "\n",
        "\n",
        "# Kernel for the morphological filtering\n",
        "kernel = cv2.getStructuringElement(1,(17,17))\n",
        "\n",
        "# Perform the blackHat filtering on the grayscale image to find the\n",
        "# hair countours\n",
        "blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
        "cv2.imshow('black',blackhat)\n",
        "cv2.waitKey(0)\n",
        "#cv2.imwrite('C://Users//Admin//Documents//blackhat_sample1.jpg', blackhat, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
        "\n",
        "# intensify the hair countours in preparation for the inpainting\n",
        "# algorithm\n",
        "ret,thresh2 = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n",
        "cv2.imshow('thresh',thresh2)\n",
        "cv2.waitKey(0)\n",
        "#cv2.imwrite('C://Users//Admin//Documents//thresholded_sample1.jpg', thresh2, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n",
        "\n",
        "# inpaint the original image depending on the mask\n",
        "dst = cv2.inpaint(src,thresh2,1,cv2.INPAINT_TELEA)\n",
        "cv2.imshow('dest',dst)\n",
        "cv2.waitKey(0)\n",
        "#cv2.imwrite('C:/Users/Admin/Documents/InPainted_sample1.jpg', dst, [int(cv2.IMWRITE_JPEG_QUALITY),Â 90])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05166e35",
      "metadata": {
        "id": "05166e35"
      },
      "outputs": [],
      "source": [
        "#python code for increasing resolution\n",
        "import cv2\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('input_image.jpg')\n",
        "\n",
        "# Define the new size of the image\n",
        "new_width = 2 * image.shape[1]\n",
        "new_height = 2 * image.shape[0]\n",
        "new_size = (new_width, new_height)\n",
        "\n",
        "# Apply bicubic interpolation\n",
        "resized_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Save the new image\n",
        "cv2.imwrite('output_image.jpg', resized_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40909e33",
      "metadata": {
        "id": "40909e33"
      },
      "outputs": [],
      "source": [
        "#python code using MCACNN\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_mcan_model():\n",
        "    inputs = layers.Input(shape=(None, None, 3))\n",
        "\n",
        "    # First stage\n",
        "    x = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    x = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Second stage\n",
        "    x = layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Third stage\n",
        "    x = layers.Conv2D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Fourth stage\n",
        "    x = layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Fifth stage\n",
        "    x = layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Output stage\n",
        "    outputs = layers.Conv2D(filters=3, kernel_size=3, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Instantiate the MCAN model\n",
        "mcan_model = build_mcan_model()\n",
        "\n",
        "# Compile the model with Adam optimizer and mean squared error loss\n",
        "mcan_model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "\n",
        "# Train the model on your image data\n",
        "mcan_model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "#In this example, the build_mcan_model() function defines the MCAN model using TensorFlow's Keras API.\n",
        "The model consists of five\n",
        "stages, where each stage contains two convolutional layers. The first four stages perform downsampling operations using\n",
        "strided convolutions, while the last two stages perform upsampling operations using transposed convolutions. The final output\n",
        "is a 3-channel image with the same dimensions as the input.\n",
        "After defining the model, you can compile it with an optimizer and a loss function. In this case, we're using the\n",
        "Adam optimizer and mean squared error (MSE) loss. Finally, you can train the model on your image data using the fit() method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa6390e",
      "metadata": {
        "id": "6aa6390e"
      },
      "outputs": [],
      "source": [
        "#python code for increasing resolution using bilateral filtering\n",
        "import cv2\n",
        "\n",
        "# Load the input image\n",
        "img = cv2.imread(\"input_image.jpg\")\n",
        "\n",
        "# Set the desired scale factor\n",
        "scale = 2\n",
        "\n",
        "# Resize the image to a larger size using bilinear interpolation\n",
        "img_resized = cv2.resize(img, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Apply bilateral filter to smooth out the resized image\n",
        "img_filtered = cv2.bilateralFilter(img_resized, 9, 75, 75)\n",
        "\n",
        "# Display the input, resized, and filtered images\n",
        "cv2.imshow(\"Input Image\", img)\n",
        "cv2.imshow(\"Resized Image\", img_resized)\n",
        "cv2.imshow(\"Filtered Image\", img_filtered)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "#In this code, we first load the image using cv2.imread() function. Then, we define the scale factor that we want to use to\n",
        "increase the resolution. In this case, we use a scale factor of 2, which means we want to double the resolution of the image.\n",
        "\n",
        "We then apply bilateral filtering to the image using cv2.bilateralFilter() function. Bilateral filtering is a non-linear filter\n",
        "that preserves edges while smoothing the image. It works by computing a weighted average of the pixel values in the neighborhood\n",
        "of each pixel, where the weights are based on both the spatial distance and the color similarity between the pixels. This helps\n",
        "to reduce noise while preserving the important features of the image.\n",
        "\n",
        "After applying bilateral filtering, we resize the image to the new resolution using cv2.resize() function. We use\n",
        "interpolation=cv2.\n",
        "INTER_LINEAR to specify the type of interpolation to use when resizing the image. In this case, we use linear interpolation,\n",
        "which works by computing a weighted average of the pixel values in the neighborhood of each pixel.\n",
        "\n",
        "Finally, we display the original and the resized image using cv2.imshow() function. We use cv2.waitKey() to wait for a key event,\n",
        "and cv2.destroyAllWindows() to destroy all the windows created by the program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdef7159",
      "metadata": {
        "id": "bdef7159"
      },
      "outputs": [],
      "source": [
        "#python code for CLAHE\n",
        "import cv2\n",
        "\n",
        "def clahe(img, clip_limit=2.0, tile_size=8):\n",
        "    \"\"\"\n",
        "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to the given image.\n",
        "\n",
        "    Parameters:\n",
        "    - img: The input image as a numpy array.\n",
        "    - clip_limit: The contrast limit for CLAHE.\n",
        "    - tile_size: The size of the tiles used for local contrast enhancement.\n",
        "\n",
        "    Returns:\n",
        "    - The CLAHE-enhanced image as a numpy array.\n",
        "    \"\"\"\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Create a CLAHE object\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile_size, tile_size))\n",
        "\n",
        "    # Apply CLAHE to the grayscale image\n",
        "    equalized = clahe.apply(gray)\n",
        "\n",
        "    # Convert the grayscale image back to BGR\n",
        "    result = cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    return result\n",
        "#You can use this function to enhance the contrast of an image as follows:\n",
        "# Load the image\n",
        "img = cv2.imread('path/to/image.jpg')\n",
        "\n",
        "# Apply CLAHE to the image\n",
        "clahe_img = clahe(img, clip_limit=2.0, tile_size=8)\n",
        "\n",
        "# Display the original and CLAHE-enhanced images\n",
        "cv2.imshow('Original Image', img)\n",
        "cv2.imshow('CLAHE-Enhanced Image', clahe_img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "#In this example, clip_limit and tile_size are the parameters that control the amount of contrast enhancement and the\n",
        "size of the tiles used for local contrast enhancement, respectively. You can experiment with different values for these\n",
        "parameters to achieve the desired level of contrast enhancement."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}